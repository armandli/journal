{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from IPython.display import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/armandli/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_img = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_built()\n",
    "if use_cuda:\n",
    "    device = torch.device('cuda')\n",
    "elif use_mps:\n",
    "    #mps has bugs that cannot handle ConvTranspose2d, reverting to cpu\n",
    "    #device = torch.device('mps')\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_batch_size = 256\n",
    "loader_args = {'batch_size' : default_batch_size, 'shuffle' : True}\n",
    "score_args = {'batch_size' : default_batch_size, 'shuffle' : False}\n",
    "if use_cuda:\n",
    "    loader_args.update({'pin_memory' : True})\n",
    "    score_args.update({'pin_memory' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reporter(ABC):\n",
    "    @abstractmethod\n",
    "    def report(self, typ, **metric):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SReporter(Reporter):\n",
    "    def __init__(self):\n",
    "        self.log = []\n",
    "    def report(self, typ, **data):\n",
    "        self.log.append((typ, data))\n",
    "    def reset(self):\n",
    "        self.log.clear()\n",
    "    def loss(self, t):\n",
    "        losses = []\n",
    "        for (typ, data) in self.log:\n",
    "            if typ == t:\n",
    "                losses.append(data['loss'])\n",
    "        return losses\n",
    "    def loss(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count -= 1\n",
    "        return float(\"inf\")\n",
    "    def eval_loss(self):\n",
    "        return self.loss('eval')\n",
    "    def train_loss(self):\n",
    "        return self.loss('train')\n",
    "    def eval_loss(self, idx):\n",
    "        return self.loss('eval', idx)\n",
    "    def train_loss(self, idx):\n",
    "        return self.loss('train', idx)\n",
    "    def get_record(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count -= 1\n",
    "        return dict()\n",
    "    def eval_record(self, idx):\n",
    "        return self.get_record('eval', idx)\n",
    "    def train_record(self, idx):\n",
    "        return self.get_record('train', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbeddingV1(nn.Module):\n",
    "    def __init__(self, cdim):\n",
    "        super(TimeEmbeddingV1, self).__init__()\n",
    "        self.cdim = cdim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.cdim//4, self.cdim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.cdim, self.cdim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        half_dim = self.cdim // 8\n",
    "        emb = math.log(10_000) / (half_dim-1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:,None] * emb[None,:]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "        emb = self.layers(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockV1(nn.Module):\n",
    "    def __init__(self, cdim_in, cdim_out, tdim, gdim=32, dropout=0.1):\n",
    "        super(ResidualBlockV1, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.GroupNorm(gdim, cdim_in),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(cdim_in, cdim_out, kernel_size=(3,3), padding=(1,1)),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.GroupNorm(gdim, cdim_out),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Conv2d(cdim_out, cdim_out, kernel_size=(3,3), padding=(1,1)),\n",
    "        )\n",
    "        self.time_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(tdim, cdim_out),\n",
    "        )\n",
    "        if cdim_in != cdim_out:\n",
    "            self.skip = nn.Conv2d(cdim_in, cdim_out, kernel_size=(1,1))\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        h = self.layer1(x)\n",
    "        h += self.time_layer(t)[:,:,None,None]\n",
    "        h = self.layer2(h)\n",
    "        return h + self.skip(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlockV1(nn.Module):\n",
    "    def __init__(self, cdim, hdim=1, d_k=None, gdim=32):\n",
    "        super(AttentionBlockV1, self).__init__()\n",
    "        if d_k is None:\n",
    "            d_k = cdim\n",
    "        self.proj = nn.Linear(cdim, hdim * d_k * 3)\n",
    "        self.output = nn.Linear(hdim * d_k, cdim)\n",
    "        self.hdim = hdim\n",
    "        self.d_k = d_k\n",
    "        self.scale = d_k ** -0.5\n",
    "    \n",
    "    #NOTE: t is not used\n",
    "    def forward(self, x, t=None):\n",
    "        batch_sz, cdim, h, w = x.shape\n",
    "        x = x.view(batch_sz, cdim, -1).permute(0, 2, 1)\n",
    "        qkv = self.proj(x).view(batch_sz, -1, self.hdim, 3 * self.d_k)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        attn = attn.softmax(dim=2)\n",
    "        res = torch.einsum('bihj,bjhd->bihd', attn, v)\n",
    "        res = res.view(batch_sz, -1, self.hdim * self.d_k)\n",
    "        res = self.output(res)\n",
    "        res += x\n",
    "        res = res.permute(0, 2, 1).view(batch_sz, cdim, h, w)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlockV1(nn.Module):\n",
    "    def __init__(self, cdim_in, cdim_out, tdim, has_attn):\n",
    "        super(DownBlockV1, self).__init__()\n",
    "        self.res = ResidualBlockV1(cdim_in, cdim_out, tdim)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlockV1(cdim_out)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlockV1(nn.Module):\n",
    "    def __init__(self, cdim_in, cdim_out, tdim, has_attn):\n",
    "        super(UpBlockV1, self).__init__()\n",
    "        self.res = ResidualBlockV1(cdim_in + cdim_out, cdim_out, tdim)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlockV1(cdim_out)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiddleBlockV1(nn.Module):\n",
    "    def __init__(self, cdim, tdim):\n",
    "        super(MiddleBlockV1, self).__init__()\n",
    "        self.res1 = ResidualBlockV1(cdim, cdim, tdim)\n",
    "        self.attn = AttentionBlockV1(cdim)\n",
    "        self.res2 = ResidualBlockV1(cdim, cdim, tdim)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleV1(nn.Module):\n",
    "    def __init__(self, cdim):\n",
    "        super(UpSampleV1, self).__init__()\n",
    "        self.layer = nn.ConvTranspose2d(cdim, cdim, (4,4), (2,2), (1,1))\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSampleV1(nn.Module):\n",
    "    def __init__(self, cdim):\n",
    "        super(UpSampleV1, self).__init__()\n",
    "        self.layer = nn.Conv2d(cdim, cdim, (3,3), (2,2), (1,1))\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMV1(nn.Module):\n",
    "    def __init__(self, cdim_in, cdim, cmults, is_attn, n_blocks):\n",
    "        super(DDPMV1, self).__init__()\n",
    "        n = len(cmults)\n",
    "        self.time_emb = TimeEmbeddingV1(cdim*4)\n",
    "        self.image_proj = nn.Conv2d(cdim_in, cdim, kernel_size=(3,3), padding=(1,1))\n",
    "        down = []\n",
    "        out_channels = in_channels = cdim\n",
    "        for i in range(n):\n",
    "            out_channels = in_channels * cmults[i]\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlockV1(in_channels, out_channels, cdim*4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            down.append(DownSampleV1(in_channels))\n",
    "        self.down = nn.ModuleList(down)\n",
    "        self.middle = MiddleBlockV1(out_channels, cdim*4)\n",
    "        up = []\n",
    "        in_channels = out_channels\n",
    "        for i in reversed(range(n)):\n",
    "            up.append(UpSampleV1(in_channels))\n",
    "            out_channels = in_channels // cmults[i]\n",
    "            up.append(UpBlockV1(in_channels+out_channels*cmults[i], out_channels, cdim*4, is_attn[i]))\n",
    "            for _ in range(1, n_blocks):\n",
    "                up.append(UpBlockV1(out_channels, out_channels, cdim*4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "        self.up = nn.ModuleList(up)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(8, cdim),\n",
    "            nn.SilU(),\n",
    "            nn.Conv2d(in_channels, cdim_in, kernel_size=(3,3), padding=(1,1)),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = self.time_emb(t)\n",
    "        x = self.image_proj(x)\n",
    "        h = []\n",
    "        for m in self.down:\n",
    "            if isinstance(m, DownSampleV1):\n",
    "                h.append(x)\n",
    "            x = m(x, t)\n",
    "        x = self.middle(x, t)\n",
    "        is_first = False\n",
    "        for m in self.up:\n",
    "            if isinstance(m, UpSampleV1):\n",
    "                x = m(x, t)\n",
    "                is_first = True\n",
    "            else:\n",
    "                if is_first:\n",
    "                    s = h.pop()\n",
    "                    x = torch.cat((x, s), dim=1)\n",
    "                    is_first = False\n",
    "                x = m(x, t)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
