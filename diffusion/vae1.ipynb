{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/armandli/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_built()\n",
    "if use_cuda:\n",
    "    device = torch.device('cuda')\n",
    "elif use_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_batch_size = 256\n",
    "loader_args = {'batch_size' : default_batch_size, 'shuffle' : True}\n",
    "score_args = {'batch_size' : default_batch_size, 'shuffle' : False}\n",
    "if use_cuda:\n",
    "    loader_args.update({'pin_memory' : True})\n",
    "    score_args.update({'pin_memory' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reporter(ABC):\n",
    "    @abstractmethod\n",
    "    def report(self, typ, **metric):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SReporter(Reporter):\n",
    "    def __init__(self):\n",
    "        self.log = []\n",
    "    def report(self, typ, **data):\n",
    "        self.log.append((typ, data))\n",
    "    def reset(self):\n",
    "        self.log.clear()\n",
    "    def loss(self, t):\n",
    "        losses = []\n",
    "        for (typ, data) in self.log:\n",
    "            if typ == t:\n",
    "                losses.append(data['loss'])\n",
    "        return losses\n",
    "    def loss(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count -= 1\n",
    "        return float(\"inf\")\n",
    "    def eval_loss(self):\n",
    "        return self.loss('eval')\n",
    "    def train_loss(self):\n",
    "        return self.loss('train')\n",
    "    def eval_loss(self, idx):\n",
    "        return self.loss('eval', idx)\n",
    "    def train_loss(self, idx):\n",
    "        return self.loss('train', idx)\n",
    "    def get_record(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count -= 1\n",
    "        return dict()\n",
    "    def eval_record(self, idx):\n",
    "        return self.get_record('eval', idx)\n",
    "    def train_record(self, idx):\n",
    "        return self.get_record('train', idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, target, mu, sig):\n",
    "        recon_loss = ((target - pred)**2.).sum()\n",
    "        dkl_loss = (sig**2. + mu**2. - torch.log(sig) - 0.5).sum()\n",
    "        return recon_loss + dkl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Exp, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.exp(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoderV1(nn.Module):\n",
    "    def __init__(self, idim, hdim, zdim):\n",
    "        super(VariationalEncoderV1, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(idim, hdim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(hdim, zdim)\n",
    "        self.sig_layer = nn.Sequential(\n",
    "            nn.Linear(hdim, zdim),\n",
    "            Exp(), #NOTE: this could explode, need to regularize it\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.layer1(x)\n",
    "        mu = self.mu_layer(x)\n",
    "        sig = self.sig_layer(x)\n",
    "        if torch.isnan(sig).sum() > 0:\n",
    "            print(f\"NaN in sig\")\n",
    "        return (mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoderV2(nn.Module):\n",
    "    def __init__(self, idim, hdim, zdim):\n",
    "        super(VariationalEncoderV2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(idim, hdim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(hdim, zdim)\n",
    "        self.sig_layer = nn.Sequential(\n",
    "            nn.Linear(hdim, zdim),\n",
    "            nn.Softplus(threshold=6),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.layer1(x)\n",
    "        mu = self.mu_layer(x)\n",
    "        sig = self.sig_layer(x)\n",
    "        if torch.isnan(sig).sum() > 0:\n",
    "            print(f\"NaN in sig!\")\n",
    "        return (mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderV1(nn.Module):\n",
    "    def __init__(self, idim, hdim, zdim):\n",
    "        super(DecoderV1, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(zdim, hdim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hdim, idim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoderV1(nn.Module):\n",
    "    def __init__(self, idim, hdim, zdim, dist):\n",
    "        super(VariationalAutoEncoderV1, self).__init__()\n",
    "        self.encoder = VariationalEncoderV2(idim, hdim, zdim)\n",
    "        self.decoder = DecoderV1(idim, hdim, zdim)\n",
    "        self.dist = dist\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        mu, sig = self.encoder(x)\n",
    "        s = self.dist.sample(mu.shape).to(device)\n",
    "        z = mu + sig * s\n",
    "        x_h = self.decoder(z)\n",
    "        return (x_h, mu, sig)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, sig = self.encoder(x)\n",
    "        return (mu, sig)\n",
    "\n",
    "    def decode(self, mu, sig, device):\n",
    "        s = self.dist.sample(mu.shape).to(device)\n",
    "        z = mu + sig * s\n",
    "        x_h = self.decoder(z)\n",
    "        return x_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_train(model, device, loader, optimizer, loss, epoch, reporter):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        x_h, mu, sig = model(x, device)\n",
    "        x_h = x_h.reshape(x.shape)\n",
    "        l = loss(x_h, x, mu, sig)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += l.item()\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='train', loss=total_loss)\n",
    "    print(f\"Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_validate(model, device, loader, loss, train_epoch, reporter):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            x_h, mu, sig = model(x, device)\n",
    "            x_h = x_h.reshape(x.shape)\n",
    "            total_loss += loss(x_h, x, mu, sig)\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='eval', loss=total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_train_validate(\n",
    "        model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        eval_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        total_epoch,\n",
    "        patience,\n",
    "        patience_decay,\n",
    "        reporter,\n",
    "):\n",
    "    validation_loss = float(\"inf\")\n",
    "    patience_count = patience\n",
    "    patience = int(patience * patience_decay)\n",
    "    reset_patience = False\n",
    "    for epoch in range(total_epoch):\n",
    "        vae_image_train(model, device, train_loader, optimizer, loss, epoch, reporter)\n",
    "        vae_image_validate(model, device, eval_loader, loss, epoch, reporter)\n",
    "        new_validation_loss = reporter.eval_loss(-1)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {new_validation_loss}\")\n",
    "        scheduler.step(new_validation_loss)\n",
    "        if new_validation_loss < validation_loss:\n",
    "            validation_loss = new_validation_loss\n",
    "            patience_count = patience\n",
    "            if reset_patience:\n",
    "                patience = int(patience * patience_decay)\n",
    "                reset_patience = False\n",
    "        else:\n",
    "            validation_loss = new_validation_loss\n",
    "            patience_count -= 1\n",
    "            reset_patience = True\n",
    "            if patience_count <= 0:\n",
    "                print(f\"Improvement stopped. Validation Loss: {validation_loss}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset  = datasets.MNIST(root=data_dir, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset = datasets.CIFAR10(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR100(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset = datasets.CIFAR100(root=data_dir, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = torch.flatten(trainset[0][0]).shape[0]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(dataset=trainset, **loader_args)\n",
    "eval_loader = utils.data.DataLoader(dataset=evalset, **score_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dist = distributions.Normal(0, 1)\n",
    "model = VariationalAutoEncoderV1(input_dim, 1024, 512, norm_dist)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "total_epochs  = 60\n",
    "patience      = 8\n",
    "patience_decay= 0.9\n",
    "optimizer     = optim.Adam(model.parameters(recurse=True), lr=learning_rate)\n",
    "scheduler     = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience/4, threshold=0.01)\n",
    "loss          = VAELoss()\n",
    "reporter      = SReporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 361.0521227734375\n",
      "Epoch 0 Validation Loss: 331.14117431640625\n",
      "Train Loss: 322.92201140625\n",
      "Epoch 1 Validation Loss: 319.2463073730469\n",
      "Train Loss: 314.0591126953125\n",
      "Epoch 2 Validation Loss: 309.1727294921875\n",
      "Train Loss: 307.99328328125\n",
      "Epoch 3 Validation Loss: 306.78900146484375\n",
      "Train Loss: 303.355739765625\n",
      "Epoch 4 Validation Loss: 299.70318603515625\n",
      "Train Loss: 297.8630059375\n",
      "Epoch 5 Validation Loss: 296.04901123046875\n",
      "Train Loss: 294.92276875\n",
      "Epoch 6 Validation Loss: 292.6917724609375\n",
      "Train Loss: 291.7872937109375\n",
      "Epoch 7 Validation Loss: 290.6287536621094\n",
      "Train Loss: 290.114244765625\n",
      "Epoch 8 Validation Loss: 289.5167236328125\n",
      "Train Loss: 289.2089483984375\n",
      "Epoch 9 Validation Loss: 288.3432312011719\n",
      "Train Loss: 288.2650948046875\n",
      "Epoch 10 Validation Loss: 287.5272216796875\n",
      "Train Loss: 287.03495046875\n",
      "Epoch 11 Validation Loss: 288.11639404296875\n",
      "Train Loss: 285.741744765625\n",
      "Epoch 12 Validation Loss: 285.5701599121094\n",
      "Train Loss: 285.5082698828125\n",
      "Epoch 13 Validation Loss: 285.3618469238281\n",
      "Train Loss: 285.202792109375\n",
      "Epoch 14 Validation Loss: 285.1932678222656\n",
      "Train Loss: 284.9942705078125\n",
      "Epoch 15 Validation Loss: 284.9447326660156\n",
      "Train Loss: 284.84879046875\n",
      "Epoch 16 Validation Loss: 284.8052978515625\n",
      "Train Loss: 284.721192109375\n",
      "Epoch 17 Validation Loss: 284.8512878417969\n",
      "Train Loss: 284.7794529296875\n",
      "Epoch 18 Validation Loss: 284.8204345703125\n",
      "Train Loss: 284.771962109375\n",
      "Epoch 19 Validation Loss: 284.8317565917969\n",
      "Train Loss: 284.83724484375\n",
      "Epoch 20 Validation Loss: 284.8243408203125\n",
      "Train Loss: 284.8190146484375\n",
      "Epoch 21 Validation Loss: 284.8265075683594\n",
      "Train Loss: 284.756736953125\n",
      "Epoch 22 Validation Loss: 284.8003845214844\n",
      "Train Loss: 284.7712226953125\n",
      "Epoch 23 Validation Loss: 284.8103332519531\n",
      "Train Loss: 284.7312296875\n",
      "Epoch 24 Validation Loss: 284.8146057128906\n",
      "Train Loss: 284.7467152734375\n",
      "Epoch 25 Validation Loss: 284.7886657714844\n",
      "Train Loss: 284.849843203125\n",
      "Epoch 26 Validation Loss: 284.83929443359375\n",
      "Train Loss: 284.8250419140625\n",
      "Epoch 27 Validation Loss: 284.8493347167969\n",
      "Train Loss: 284.8824669921875\n",
      "Epoch 28 Validation Loss: 284.7103576660156\n",
      "Train Loss: 284.85560734375\n",
      "Epoch 29 Validation Loss: 284.8398132324219\n"
     ]
    }
   ],
   "source": [
    "vae_image_train_validate(model, device, train_loader, eval_loader, optimizer, scheduler, loss, total_epochs, patience, patience_decay, reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_img = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = evalset[64][0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = evalset[64][0]\n",
    "x = x.reshape(1, 3, 32, 32)\n",
    "x_h, _, _ = model(x.to(device), device)\n",
    "x_h = x_h.to(device)\n",
    "x_h = x_h.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "to_img(x_h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_img(x.reshape(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image using mu and sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
