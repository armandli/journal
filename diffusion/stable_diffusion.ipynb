{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stable diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/armandli/data/'\n",
    "model_dir = '/Users/armandli/journal/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_built()\n",
    "if use_cuda:\n",
    "    device = torch.device('cuda')\n",
    "elif use_mps:\n",
    "    device = torch.device('cpu') # some mps procedure does not work\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_batch_size = 256\n",
    "loader_args = {'batch_size' : default_batch_size, 'shuffle' : True}\n",
    "score_args = {'batch_size' : default_batch_size, 'shuffle' : False}\n",
    "if use_cuda:\n",
    "    loader_args.update({'pin_memory' : True})\n",
    "    score_args.update({'pin_memory' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reporter(ABC):\n",
    "    @abstractmethod\n",
    "    def report(self, typ, **metric):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SReporter(Reporter):\n",
    "    def __init__(self):\n",
    "        self.log = []\n",
    "    def report(self, typ, **data):\n",
    "        self.log.append((typ, data))\n",
    "    def reset(self):\n",
    "        self.log.clear()\n",
    "    def loss(self, t):\n",
    "        losses = []\n",
    "        for (typ, data) in self.log:\n",
    "            if typ == t:\n",
    "                losses.append(data['loss'])\n",
    "        return losses\n",
    "    def loss(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count -= 1\n",
    "        return float(\"inf\")\n",
    "    def eval_loss(self):\n",
    "        return self.loss('eval')\n",
    "    def train_loss(self):\n",
    "        return self.loss('train')\n",
    "    def eval_loss(self, idx):\n",
    "        return self.loss('eval', idx)\n",
    "    def train_loss(self, idx):\n",
    "        return self.loss('train', idx)\n",
    "    def get_record(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count -= 1\n",
    "        return dict()\n",
    "    def eval_record(self, idx):\n",
    "        return self.get_record('eval', idx)\n",
    "    def train_record(self, idx):\n",
    "        return self.get_record('train', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset  = datasets.MNIST(root=data_dir, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset = datasets.CIFAR10(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR100(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset = datasets.CIFAR100(root=data_dir, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = torch.flatten(trainset[0][0]).shape[0]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(dataset=trainset, **loader_args)\n",
    "eval_loader = utils.data.DataLoader(dataset=evalset, **score_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedMixLogisticLoss(nn.Module):\n",
    "    def __init__(self, nmix):\n",
    "        super(DiscretizedMixLogisticLoss, self).__init__()\n",
    "        self.nmix = nmix\n",
    "    \n",
    "    def log_sum_exp(self, x):\n",
    "        axis = len(x.shape)-1\n",
    "        m, _ = torch.max(x, dim=axis)\n",
    "        n, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "        return m + torch.log(torch.sum(torch.exp(x-n), dim=axis))\n",
    "\n",
    "    def log_prob_from_logits(self, x):\n",
    "        axis = len(x.shape)-1\n",
    "        m, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "        return x - m - torch.log(torch.sum(torch.exp(x-m), dim=axis, keepdim=True))\n",
    "    \n",
    "    def forward(self, target, prediction, device):\n",
    "        nmix = self.nmix\n",
    "        target = target.permute(0, 2, 3, 1)\n",
    "        prediction = prediction.permute(0, 2, 3, 1)\n",
    "        ts = list(target.shape)\n",
    "\n",
    "        # unpack prediction parameters\n",
    "        lp = prediction[:,:,:,:nmix]\n",
    "        prediction = prediction[:,:,:,nmix:].view(ts+[nmix*3]) # 3 for mean, scale, coeff\n",
    "        means = prediction[:,:,:,:,:nmix]\n",
    "        log_scales = torch.clamp(prediction[:,:,:,:,nmix:nmix*2], min=-7.)\n",
    "        coeffs = torch.tanh(prediction[:,:,:,:,nmix*2:nmix*3])\n",
    "        target = target.unsqueeze(-1) + torch.zeros(ts+[nmix], requires_grad=False).to(device)\n",
    "        \n",
    "        m2 = (means[:,:,:,1,:]+coeffs[:,:,:,0,:]*target[:,:,:,0,:]).view(ts[0],ts[1],ts[2],1,nmix)\n",
    "        m3 = (means[:,:,:,2,:]+coeffs[:,:,:,1,:]*target[:,:,:,0,:]+coeffs[:,:,:,2,:]*target[:,:,:,1,:]).view(ts[0],ts[1],ts[2],1,nmix)\n",
    "        \n",
    "        centered_target = target - torch.cat((means[:,:,:,0,:].unsqueeze(3), m2, m3), dim=3)\n",
    "        inv_stdv  = torch.exp(-log_scales)\n",
    "        plus_in   = inv_stdv * (centered_target + 1./255.)\n",
    "        cdf_plus  = torch.sigmoid(plus_in)\n",
    "        min_in    = inv_stdv * (centered_target - 1./255.)\n",
    "        cdf_minus = torch.sigmoid(min_in)\n",
    "\n",
    "        log_cdf_plus          = plus_in - F.softplus(plus_in)\n",
    "        log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "        cdf_delta             = cdf_plus - cdf_minus\n",
    "        log_pdf_mid = (inv_stdv*centered_target) - log_scales - 2.*F.softplus(inv_stdv*centered_target)\n",
    "\n",
    "        inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "        inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1-inner_inner_cond)*(log_pdf_mid-math.log(127.5))\n",
    "        inner_cond       = (target > 0.999).float()\n",
    "        inner_out        = inner_cond*log_one_minus_cdf_min + (1.-inner_cond)*inner_inner_out\n",
    "        cond             = (target < -0.999).float()\n",
    "        log_probs        = cond*log_cdf_plus + (1.-cond)*inner_out\n",
    "        log_probs        = torch.sum(log_probs, dim=3) + self.log_prob_from_logits(lp)\n",
    "        return -torch.sum(self.log_sum_exp(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelVAELoss(nn.Module):\n",
    "    def __init__(self, nmix):\n",
    "        super(PixelVAELoss, self).__init__()\n",
    "        self.mixlogloss = DiscretizedMixLogisticLoss(nmix)\n",
    "    \n",
    "    def forward(self, target, prediction, mu, sig, device):\n",
    "        recon_loss = self.mixlogloss(target, prediction, device)\n",
    "        dkl_loss = (sig**2. + mu**2. - torch.log(sig) - 0.5).sum()\n",
    "        return recon_loss + dkl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation():\n",
    "    return nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling2DV2(in_c, out_c, stride, norm_layer):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 1, stride=stride),\n",
    "        norm_layer(out_c),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling2DV1(in_c, out_c, stride, norm_layer):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_c, out_c, 2, stride=stride),\n",
    "        norm_layer(out_c),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer2DV4(nn.Module):\n",
    "    def __init__(self, in_c, out_c, ksz, act_layer, norm_layer, stride=1):\n",
    "        super(ResidualLayer2DV4, self).__init__()\n",
    "        if in_c <= out_c:\n",
    "            self.c1 = nn.Conv2d(in_c, out_c, ksz, stride=stride, padding=int((ksz-1)/2))\n",
    "            self.c2 = nn.Conv2d(out_c, out_c, ksz, stride=1, padding=int((ksz-1)/2))\n",
    "        else:\n",
    "            self.c1 = nn.ConvTranspose2d(in_c, out_c, ksz+1, stride=stride, padding=int((ksz-1)/2))\n",
    "            self.c2 = nn.ConvTranspose2d(out_c, out_c, ksz, stride=1, padding=int((ksz-1)/2))\n",
    "        self.a1 = act_layer()\n",
    "        self.a2 = act_layer()\n",
    "        self.b1 = norm_layer(in_c)\n",
    "        self.b2 = norm_layer(out_c)\n",
    "        \n",
    "        if in_c < out_c:\n",
    "            self.residual = downsampling2DV2(in_c, out_c, stride, norm_layer)\n",
    "        elif in_c > out_c:\n",
    "            self.residual = upsampling2DV1(in_c, out_c, stride, norm_layer)\n",
    "        elif stride > 1:\n",
    "            self.residual = downsampling2DV2(in_c, out_c, stride, norm_layer)\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.c1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.c2(x)\n",
    "        s = self.residual(s)\n",
    "        x = x + s\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVariationalEncoderV2(nn.Module):\n",
    "    def __init__(self, ic, chmuls, hmul):\n",
    "        super(ConvVariationalEncoderV2, self).__init__()\n",
    "        layer1 = []\n",
    "        outmul = 1\n",
    "        for mul in chmuls:\n",
    "            layer1.append(ResidualLayer2DV4(ic*outmul, ic*mul, 3, relu_activation, nn.BatchNorm2d, stride=2))\n",
    "            outmul = mul\n",
    "        self.layer1 = nn.ModuleList(layer1)\n",
    "        self.mu_layer = nn.Conv2d(ic*outmul, ic*hmul, (3,3), (2,2), (1,1))\n",
    "        self.sig_layer = nn.Sequential(\n",
    "            nn.Conv2d(ic*outmul, ic*hmul, (3,3), (2,2), (1,1)),\n",
    "            nn.Softplus(threshold=6),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layer1:\n",
    "            x = layer(x)\n",
    "        mu = self.mu_layer(x)\n",
    "        sig = self.sig_layer(x)\n",
    "        return (mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "    def forward(self, x):\n",
    "        a, b = torch.chunk(x, 2, dim=1)\n",
    "        return a * torch.sigmoid(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatELU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        #concat at channel dim\n",
    "        return F.elu(torch.cat([x, -x], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormLinear2d(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(WeightNormLinear2d, self).__init__()\n",
    "        self.layer = nn.utils.parametrizations.weight_norm(nn.Linear(d_in, d_out))\n",
    "        self.d_out = d_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        shape = [int(d) for d in x.shape]\n",
    "        x = self.layer(x.contiguous().view(shape[0]*shape[1]*shape[2], shape[3]))\n",
    "        shape[-1] = self.d_out\n",
    "        x = x.view(shape).permute(0, 3, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride=1, padding=0):\n",
    "        super(WeightNormConv2d, self).__init__()\n",
    "        self.layer = nn.utils.parametrizations.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormConvTransposed2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, output_padding=1):\n",
    "        super(WeightNormConvTransposed2d, self).__init__()\n",
    "        self.layer = nn.utils.parametrizations.weight_norm(nn.ConvTranspose2d(in_c, out_c, kernel_size, stride, output_padding=output_padding))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownShift(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DownShift, self).__init__()\n",
    "        #pad Left=0 Right=0 Up=1 Down=0\n",
    "        self.pad = nn.ZeroPad2d((0,0,1,0))\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = x[:, :, :shape[2]-1, :]\n",
    "        x = self.pad(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownShiftConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,3), stride=(1,1), shift_down=False):\n",
    "        super(DownShiftConv2d, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ZeroPad2d((int((kernel_size[1]-1)/2),int((kernel_size[1]-1)/2),kernel_size[0]-1,0)),\n",
    "            WeightNormConv2d(in_c, out_c, kernel_size, stride),\n",
    "        )\n",
    "        if shift_down:\n",
    "            self.shift_down = DownShift()\n",
    "        else:\n",
    "            self.shift_down = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.shift_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownShiftDeconv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,3), stride=(2,2)):\n",
    "        super(DownShiftDeconv2d, self).__init__()\n",
    "        self.ks = kernel_size\n",
    "        self.layer = WeightNormConvTransposed2d(in_c, out_c, kernel_size, stride, output_padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        s = x.shape\n",
    "        # correct the shape because TransposedConv2d would produce a few rows and columns bigger\n",
    "        x = x[:, :, :(s[2]-self.ks[0]+1), int((self.ks[1]-1)/2):(s[3]-int((self.ks[1]-1)/2))]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightShift(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RightShift, self).__init__()\n",
    "        #pad Left=1 Right=0 Up=0 Down=0\n",
    "        self.pad = nn.ZeroPad2d((1,0,0,0))\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = x[:, :, :, :shape[3]-1]\n",
    "        x = self.pad(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownRightShiftConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,2), stride=(1,1), shift_right=False):\n",
    "        super(DownRightShiftConv2d, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ZeroPad2d((kernel_size[1]-1, 0, kernel_size[0]-1, 0)),\n",
    "            WeightNormConv2d(in_c, out_c, kernel_size, stride),\n",
    "        )\n",
    "        if shift_right:\n",
    "            self.shift_right = RightShift()\n",
    "        else:\n",
    "            self.shift_right = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.shift_right(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownRightShiftDeconv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,2), stride=(2,2)):\n",
    "        super(DownRightShiftDeconv2d, self).__init__()\n",
    "        self.ks = kernel_size\n",
    "        self.layer = WeightNormConvTransposed2d(in_c, out_c, kernel_size, stride, output_padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        s = x.shape\n",
    "        # correct the shape because TransposedConv2d produces a few rows and columns bigger\n",
    "        x = x[:, :, :(s[2]-self.ks[0]+1):, :(s[3]-self.ks[1]+1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualLayer(nn.Module):\n",
    "    def __init__(self, nc, conv, skip=0, p_dropout=0.5):\n",
    "        super(GatedResidualLayer, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            conv(2*nc, nc),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            nn.Dropout2d(p_dropout),\n",
    "            conv(2*nc, 2*nc),\n",
    "            Gate(),\n",
    "        )\n",
    "        if skip > 0:\n",
    "            self.skip = nn.Sequential(\n",
    "                ConcatELU(),\n",
    "                WeightNormLinear2d(2*skip*nc, nc),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, a=None):\n",
    "        s = x\n",
    "        x = self.layer1(x)\n",
    "        if a is not None:\n",
    "            x += self.skip(a)\n",
    "        x = self.layer2(x)\n",
    "        return s + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelUpSample(nn.Module):\n",
    "    def __init__(self, nlayers, nchannel):\n",
    "        super(PixelUpSample, self).__init__()\n",
    "        self.up_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownShiftConv2d, skip=0)\n",
    "            for _ in range(nlayers)\n",
    "        ])\n",
    "        self.upleft_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownRightShiftConv2d, skip=1)\n",
    "            for _ in range(nlayers)\n",
    "        ])\n",
    "        self.nlayers = nlayers\n",
    "    def forward(self, up, upleft):\n",
    "        ups, uplefts = [], []\n",
    "        for i in range(self.nlayers):\n",
    "            up = self.up_stream[i](up)\n",
    "            upleft = self.upleft_stream[i](upleft, a=up)\n",
    "            ups.append(up)\n",
    "            uplefts.append(upleft)\n",
    "        return ups, uplefts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDownSample(nn.Module):\n",
    "    def __init__(self, nlayer, nchannel):\n",
    "        super(PixelDownSample, self).__init__()\n",
    "        self.up_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownShiftConv2d, skip=1)\n",
    "            for _ in range(nlayer)\n",
    "        ])\n",
    "        self.upleft_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownRightShiftConv2d, skip=2)\n",
    "            for _ in range(nlayer)\n",
    "        ])\n",
    "        self.nlayer = nlayer\n",
    "    \n",
    "    def forward(self, up, upleft, ups, uplefts):\n",
    "        for i in range(self.nlayer):\n",
    "            up = self.up_stream[i](up, a=ups.pop())\n",
    "            upleft = self.upleft_stream[i](upleft, a=torch.cat((up, uplefts.pop()), 1))\n",
    "        return up, upleft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, in_c, nresnet, nlayer, nchannel=80, nlogmix=10):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        down_nlayer = [nresnet] + [nresnet+1 for _ in range(1, nlayer)]\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            PixelDownSample(down_nlayer[i], nchannel) for i in range(nlayer)\n",
    "        ])\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            PixelUpSample(nresnet, nchannel) for _ in range(nlayer)\n",
    "        ])\n",
    "        self.downsize_up_stream = nn.ModuleList([\n",
    "            DownShiftConv2d(nchannel, nchannel, stride=(2,2)) for _ in range((nlayer-1))\n",
    "        ])\n",
    "        self.downsize_upleft_stream = nn.ModuleList([\n",
    "            DownRightShiftConv2d(nchannel, nchannel, stride=(2,2)) for _ in range(nlayer-1)\n",
    "        ])\n",
    "        self.upsize_up_stream = nn.ModuleList([\n",
    "            DownShiftDeconv2d(nchannel, nchannel, stride=(2,2)) for _ in range(nlayer-1)\n",
    "        ])\n",
    "        self.upsize_upleft_stream = nn.ModuleList([\n",
    "            DownRightShiftDeconv2d(nchannel, nchannel, stride=(2,2)) for _ in range(nlayer-1)\n",
    "        ])\n",
    "        self.up_init = DownShiftConv2d(in_c+1, nchannel, kernel_size=(2,3), shift_down=True)\n",
    "        self.upleft_init = nn.ModuleList([\n",
    "            DownShiftConv2d(in_c+1, nchannel, kernel_size=(1,3), shift_down=True),\n",
    "            DownRightShiftConv2d(in_c+1, nchannel, kernel_size=(2,1), shift_right=True),\n",
    "        ])\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            WeightNormLinear2d(nchannel, 10*nlogmix),\n",
    "        )\n",
    "        self.nlayer = nlayer\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        shape = x.shape\n",
    "        padding = torch.ones(shape[0], 1, shape[2], shape[3], device=device, requires_grad=False)\n",
    "        x = torch.cat((x, padding), 1)\n",
    "        \n",
    "        # UP PASS\n",
    "        ups = [self.up_init(x)]\n",
    "        uplefts = [self.upleft_init[0](x) + self.upleft_init[1](x)]\n",
    "        for i in range(self.nlayer):\n",
    "            up_out, upleft_out = self.up_layers[i](ups[-1], uplefts[-1])\n",
    "            ups.extend(up_out)\n",
    "            uplefts.extend(upleft_out)\n",
    "            if i < self.nlayer-1:\n",
    "                ups.append(self.downsize_up_stream[i](ups[-1]))\n",
    "                uplefts.append(self.downsize_upleft_stream[i](uplefts[-1]))\n",
    "\n",
    "        # DOWN PASS\n",
    "        up = ups.pop()\n",
    "        upleft = uplefts.pop()\n",
    "        for i in range(self.nlayer):\n",
    "            up, upleft = self.down_layers[i](up, upleft, ups, uplefts)\n",
    "            if i < self.nlayer-1:\n",
    "                up = self.upsize_up_stream[i](up)\n",
    "                upleft = self.upsize_upleft_stream[i](upleft)\n",
    "        \n",
    "        x = self.out_layer(upleft)\n",
    "        return x\n",
    "\n",
    "    def sample(self, batch_sz, img_shape, device):\n",
    "        x = torch.zeros(batch_sz, img_shape[0], img_shape[1], img_shape[2]).to(device)\n",
    "        shape = x.shape\n",
    "        padding = torch.ones(shape[0], 1, shape[2], shape[3], device=device, requires_grad=False)\n",
    "        x = torch.cat((x, padding), 1)\n",
    "        \n",
    "        # UP PASS\n",
    "        ups = [self.up_init(x)]\n",
    "        uplefts = [self.upleft_init[0](x) + self.upleft_init[1](x)]\n",
    "        for i in range(self.nlayer):\n",
    "            up_out, upleft_out = self.up_layers[i](ups[-1], uplefts[-1])\n",
    "            ups.extend(up_out)\n",
    "            uplefts.extend(upleft_out)\n",
    "            if i < self.nlayer-1:\n",
    "                ups.append(self.downsize_up_stream[i](ups[-1]))\n",
    "                uplefts.append(self.downsize_upleft_stream[i](uplefts[-1]))\n",
    "\n",
    "        # DOWN PASS\n",
    "        up = ups.pop()\n",
    "        upleft = uplefts.pop()\n",
    "        for i in range(self.nlayer):\n",
    "            up, upleft = self.down_layers[i](up, upleft, ups, uplefts)\n",
    "            if i < self.nlayer-1:\n",
    "                up = self.upsize_up_stream[i](up)\n",
    "                upleft = self.upsize_upleft_stream[i](upleft)\n",
    "        \n",
    "        x = self.out_layer(upleft)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDecoderV1(nn.Module):\n",
    "    def __init__(self, ic, chmuls, hmul, n_res, n_layer, nmix=10):\n",
    "        super(PixelDecoderV1, self).__init__()\n",
    "        layers = []\n",
    "        outmul = hmul\n",
    "        for mul in reversed(chmuls):\n",
    "            layers.append(ResidualLayer2DV4(ic*outmul, ic*mul, 3, relu_activation, nn.BatchNorm2d, stride=2))\n",
    "            outmul = mul\n",
    "        layers.append(ResidualLayer2DV4(ic*outmul, ic, 3, relu_activation, nn.BatchNorm2d, stride=2))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.pixel = PixelCNN(in_c=3, nresnet=n_res, nlayer=n_layer, nlogmix=nmix)\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.pixel(x, device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelVariationalAutoEncoderV1(nn.Module):\n",
    "    def __init__(self, ic, chmuls, hmul, dist, n_res=1, n_layer=1, nmix=10):\n",
    "        super(PixelVariationalAutoEncoderV1, self).__init__()\n",
    "        self.encoder = ConvVariationalEncoderV2(ic, chmuls, hmul)\n",
    "        self.decoder = PixelDecoderV1(ic, chmuls, hmul, n_res, n_layer, nmix)\n",
    "        self.dist = dist\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        mu, sig = self.encode(x)\n",
    "        x_h = self.decode(mu, sig, device)\n",
    "        return (x_h, mu, sig)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, sig = self.encoder(x)\n",
    "        return (mu, sig)\n",
    "    \n",
    "    def decode(self, mu, sig, device):\n",
    "        s = self.dist.sample(mu.shape).to(device)\n",
    "        z = mu + sig * s\n",
    "        x_h = self.decoder(z, device)\n",
    "        return x_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE Encoder Decoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_train(model, device, loader, optimizer, loss, epoch, reporter):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        x_h, mu, sig = model(x, device)\n",
    "        l = loss(x, x_h, mu, sig, device)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += l.item()\n",
    "        print(f\"Epoch {epoch}: {l.item()}\")\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='train', loss=total_loss)\n",
    "    print(f\"Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_validate(model, device, loader, loss, train_epoch, reporter):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            x_h, mu, sig = model(x, device)\n",
    "            total_loss += loss(x, x_h, mu, sig, device)\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='eval', loss=total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_train_validate(\n",
    "        model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        eval_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        total_epoch,\n",
    "        patience,\n",
    "        patience_decay,\n",
    "        reporter,\n",
    "):\n",
    "    validation_loss = float(\"inf\")\n",
    "    patience_count = patience\n",
    "    patience = int(patience * patience_decay)\n",
    "    reset_patience = False\n",
    "    for epoch in range(total_epoch):\n",
    "        vae_image_train(model, device, train_loader, optimizer, loss, epoch, reporter)\n",
    "        vae_image_validate(model, device, eval_loader, loss, epoch, reporter)\n",
    "        new_validation_loss = reporter.eval_loss(-1)\n",
    "        print(f\"Epoch {epoch} VLoss: {new_validation_loss}\")\n",
    "        scheduler.step(new_validation_loss)\n",
    "        if new_validation_loss < validation_loss:\n",
    "            validation_loss = new_validation_loss\n",
    "            patience_count = patience\n",
    "            if reset_patience:\n",
    "                patience = int(patience * patience_decay)\n",
    "                reset_patience = False\n",
    "        else:\n",
    "            validation_loss = new_validation_loss\n",
    "            patience_count -= 1\n",
    "            reset_patience = True\n",
    "            if patience_count <= 0:\n",
    "                print(f\"Improvement stopped. VLoss: {validation_loss}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dist = distributions.Normal(0., 1.)\n",
    "inc = trainset[0][0].shape[0]\n",
    "nmix=10\n",
    "model = PixelVariationalAutoEncoderV1(inc, [inc*2, inc*4, inc*8], inc*16, norm_dist, n_res=2, n_layer=3, nmix=nmix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "total_epochs = 60\n",
    "patience = 16\n",
    "patience_decay = 0.9\n",
    "optimizer = optim.Adam(model.parameters(recurse=True), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, threshold=0.000001)\n",
    "loss = PixelVAELoss(nmix)\n",
    "reporter = SReporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_image_train_validate(model, device, train_loader, eval_loader, optimizer, scheduler, loss, total_epochs, patience, patience_decay, reporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load VAE Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = model_dir + 'vaev3_cifar10_v1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = distributions.Normal(0., 1.)\n",
    "nmix = 10\n",
    "inc = trainset[0][0].shape[0]\n",
    "model = PixelVariationalAutoEncoderV1(inc, chmuls=[inc*2, inc*4, inc*8], hmul=inc*16, dist=dist, n_res=1, n_layer=1, nmix=nmix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(filename, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Stable Diffusion dataset based on VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDDataset(utils.Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        self.image = image\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return self.image.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.image[idx], self.label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(dataset=trainset, **score_args)\n",
    "eval_loader = utils.data.DataLoader(dataset=evalset, **score_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sd_dataset(model, loader, num_classes):\n",
    "    list_image = []\n",
    "    list_label = []\n",
    "    for data, label in train_loader:\n",
    "        mu, sig = model.encode(data)\n",
    "        image = torch.cat((mu, sig), dim=1)\n",
    "        label_mtx = F.one_hot(label, num_classes=num_classes)\n",
    "        list_image.append(image)\n",
    "        list_label.append(label_mtx)\n",
    "    image = torch.cat(list_image, dim=0)\n",
    "    label = torch.cat(list_label, dim=0)\n",
    "    return SDDataset(image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_trainset = create_sd_dataset(model, train_loader, 10)\n",
    "d_validset = create_sd_dataset(model, eval_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_loader = utils.data.DataLoader(dataset=d_trainset, **loader_args)\n",
    "d_valid_loader = utils.data.DataLoader(dataset=d_validset, **score_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbeddingV1(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(TimeEmbeddingV1, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.n_channels // 4, self.n_channels),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(self.n_channels, self.n_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, device):\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) * (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim,device=device,dtype=torch.float32) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "        emb = self.layers(emb)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbeddingV2(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(TimeEmbeddingV2, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.n_channels, self.n_channels),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(self.n_channels, self.n_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, t, device):\n",
    "        half_dim = self.n_channels // 2\n",
    "        emb = math.log(10_000)\n",
    "        emb = torch.exp(torch.arange(half_dim,device=device,dtype=torch.float32) * -emb)\n",
    "        emb = t[:,None] * emb[None,:]\n",
    "        emb = torch.cat((emb.cos(), emb.sin()), dim=1)\n",
    "        emb = self.layers(emb)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbResidualBlockV1(nn.Module):\n",
    "    def __init__(self, nc, d_t_emb, n_group, out_c=None, p_drop=0.0):\n",
    "        super(TimeEmbResidualBlockV1, self).__init__()\n",
    "        if out_c is None:\n",
    "            out_c = nc\n",
    "        self.in_layers = nn.Sequential(\n",
    "            nn.GroupNorm(n_group, nc),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(nc, out_c, 3, padding=1),\n",
    "        )\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(d_t_emb, out_c),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            nn.GroupNorm(n_group, out_c),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "        )\n",
    "        if out_c == nc:\n",
    "            self.skip = nn.Identity()\n",
    "        else:\n",
    "            self.skip = nn.Conv2d(nc, out_c, 1)\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.in_layers(x)\n",
    "        t_emb = self.emb_layers(t_emb).type(h.dtype)\n",
    "        h = h + t_emb[:, :, None, None]\n",
    "        h = self.out_layers(h)\n",
    "        return self.skip(x) + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeGLUV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(GeGLUV1, self).__init__()\n",
    "        self.proj = nn.Linear(d_in, d_out*2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardV1(nn.Module):\n",
    "    def __init__(self, d_model, d_mult=4, dprob=0.):\n",
    "        super(FeedForwardV1, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            GeGLUV1(d_model, d_model * d_mult),\n",
    "            nn.Dropout(dprob),\n",
    "            nn.Linear(d_model * d_mult, d_model),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionV1(nn.Module):\n",
    "    def __init__(self, d_model, d_cond, n_heads, d_head, is_inplace=True):\n",
    "        super(CrossAttentionV1, self).__init__()\n",
    "        d_attn = d_head * n_heads\n",
    "        self.is_inplace = is_inplace\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = d_head ** -0.5\n",
    "        self.to_q = nn.Linear(d_model, d_attn, bias=False)\n",
    "        self.to_k = nn.Linear(d_cond, d_attn, bias=False)\n",
    "        self.to_v = nn.Linear(d_cond, d_attn, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(d_attn, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        if cond is None:\n",
    "            cond = x\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(cond)\n",
    "        v = self.to_v(cond)\n",
    "        return self.attention(q, k, v)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        q = q.view(*q.shape[:2], self.n_heads, -1)\n",
    "        k = k.view(*k.shape[:2], self.n_heads, -1)\n",
    "        v = v.view(*v.shape[:2], self.n_heads, -1)\n",
    "        attn = torch.einsum('bihd,bjhd->bhij', q, k) * self.scale\n",
    "        if self.is_inplace:\n",
    "            half = attn.shape[0] // 2\n",
    "            attn[half:] = attn[half:].softmax(dim=-1)\n",
    "            attn[:half] = attn[:half].softmax(dim=-1)\n",
    "        else:\n",
    "            attn = attn.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bjhd->bihd', attn, v)\n",
    "        out = out.reshape(*out.shape[:2], -1)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerBlockV1(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_head, d_cond):\n",
    "        super(BasicTransformerBlockV1, self).__init__()\n",
    "        self.attn1 = CrossAttentionV1(d_model, d_model, n_heads, d_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn2 = CrossAttentionV1(d_model, d_cond, n_heads, d_head)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForwardV1(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        x = self.attn1(self.norm1(x)) + x\n",
    "        x = self.attn2(self.norm2(x), cond=cond) + x\n",
    "        x = self.ff(self.norm3(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformerV1(nn.Module):\n",
    "    def __init__(self, channels, n_heads, n_layers, n_group, d_cond):\n",
    "        super(SpatialTransformerV1, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups=n_group, num_channels=channels, eps=1e-6, affine=True)\n",
    "        self.proj_in = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [BasicTransformerBlockV1(channels, n_heads, channels // n_heads, d_cond=d_cond) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.proj_out = nn.Conv2d(channels, channels, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x, cond):\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = self.norm(x)\n",
    "        x = self.proj_in(x)\n",
    "        x = x.permute(0, 2, 3, 1).view(b, h*w, c)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, cond)\n",
    "        x = x.view(b, h, w, c).permute(0, 3, 1, 2)\n",
    "        x = self.proj_out(x)\n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbSequential(nn.Sequential):\n",
    "    def forward(self, x, t_emb, cond=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimeEmbResidualBlockV1):\n",
    "                x = layer(x, t_emb)\n",
    "            elif isinstance(layer, SpatialTransformerV1):\n",
    "                x = layer(x, cond)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSampleV1(nn.Module):\n",
    "    def __init__(self, n_c):\n",
    "        super(DownSampleV1, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_c, n_c, (3,3), (2,2), (1,1))\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleV2(nn.Module):\n",
    "    def __init__(self, n_c):\n",
    "        super(UpSampleV2, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_c, n_c, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUNetV1(nn.Module):\n",
    "    #TODO: signal chmuls is list(tuple(int, bool))\n",
    "    def __init__(self, in_c, chmuls: list(), out_c, n_resblock, n_attnhead, n_tf, n_group, d_cond):\n",
    "        super(CUNetV1, self).__init__()\n",
    "        assert(len(chmuls) >= 1)\n",
    "        d_te = chmuls[-1][0]\n",
    "        n_c = chmuls[0][0]\n",
    "        self.time_emb = TimeEmbeddingV2(d_te)\n",
    "        self.down_blocks = nn.ModuleList([TimeEmbSequential(nn.Conv2d(in_c, chmuls[0][0], 3, padding=1))])\n",
    "        down_block_channels = [n_c]\n",
    "        for i in range(len(chmuls)):\n",
    "            for _ in range(n_resblock):\n",
    "                layers = [TimeEmbResidualBlockV1(n_c, d_te, n_group=n_group, out_c=chmuls[i][0])]\n",
    "                n_c = chmuls[i][0]\n",
    "                if chmuls[i][1]:\n",
    "                    layers.append(SpatialTransformerV1(n_c, n_attnhead, n_tf, n_group, d_cond))\n",
    "                self.down_blocks.append(TimeEmbSequential(*layers))\n",
    "                down_block_channels.append(n_c)\n",
    "            if i != len(chmuls)-1:\n",
    "                self.down_blocks.append(TimeEmbSequential(DownSampleV1(n_c)))\n",
    "                down_block_channels.append(n_c)\n",
    "        self.middle_block = TimeEmbSequential(\n",
    "            TimeEmbResidualBlockV1(n_c, d_te, n_group=n_group),\n",
    "            SpatialTransformerV1(n_c, n_attnhead, n_tf, n_group, d_cond),\n",
    "            TimeEmbResidualBlockV1(n_c, d_te, n_group=n_group),\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i in reversed(range(len(chmuls))):\n",
    "            for j in range(n_resblock+1):\n",
    "                layers = [TimeEmbResidualBlockV1(n_c+down_block_channels.pop(), d_te, n_group=n_group, out_c=chmuls[i][0])]\n",
    "                n_c = chmuls[i][0]\n",
    "                if chmuls[i][1]:\n",
    "                    layers.append(SpatialTransformerV1(n_c, n_attnhead, n_tf, n_group, d_cond))\n",
    "                if i != 0 and j == n_resblock:\n",
    "                    layers.append(UpSampleV2(n_c))\n",
    "                self.up_blocks.append(TimeEmbSequential(*layers))\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(n_group, n_c),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(n_c, out_c, 3, padding=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, time_step, cond, device):\n",
    "        t_emb = self.time_emb(time_step, device)\n",
    "        x_block = []\n",
    "        for module in self.down_blocks:\n",
    "            x = module(x, t_emb, cond)\n",
    "            x_block.append(x)\n",
    "        x = self.middle_block(x, t_emb, cond)\n",
    "        for module in self.up_blocks:\n",
    "            x = torch.cat([x, x_block.pop()], dim=1)\n",
    "            x = module(x, t_emb, cond)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEmbed(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(OneHotEmbed, self).__init__()\n",
    "        self.n_class = n_class\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        m = F.one_hot(x).to(device)\n",
    "        m = m.unsqueeze(-1).expand(m.shape[0], m.shape[1], self.n_class)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable Diffusion Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: need to determine the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: need to determine total number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = d_trainset[0][0].shape[1]\n",
    "n_class = 10\n",
    "model = CUNetV1(in_c=ic, chmuls=[(ic*2, True), (ic*4, True)], out_c=ic, n_resblock=1, n_attnhead=1, n_tf=1, n_group=4, d_cond=10)\n",
    "model = model.to(device)\n",
    "cond_embedder = OneHotEmbed(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1024\n",
    "learning_rate = 0.0001\n",
    "total_epochs = 1\n",
    "optimizer = optim.Adam(model.parameters(recurse=True), lr=learning_rate)\n",
    "loss = nn.MSELoss()\n",
    "reporter = SReporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.linspace(0.0001, 0.02, T)\n",
    "alpha = 1. - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "sigma2 = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_xt_x0(x0, t, alpha_bar):\n",
    "    mean = alpha_bar.gather(-1, t).reshape(-1, 1, 1, 1) ** 0.5 * x0\n",
    "    var = 1 - alpha_bar.gather(-1, t).reshape(-1, 1, 1, 1)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0, t, alpha_bar, eps=None):\n",
    "    if eps is None:\n",
    "        eps = torch.randn_like(x0)\n",
    "    mean, var = q_xt_x0(x0, t, alpha_bar)\n",
    "    return mean + (var ** 0.5) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample(xt, eps_theta, device, t, alpha_bars, alphas, sigma2s):\n",
    "    noise = torch.randn(xt.shape, device=device) if t != 0 else torch.zeros(xt.shape)\n",
    "    alpha_bar = alpha_bars.gather(-1, t)\n",
    "    alpha = alphas.gather(-1, t)\n",
    "    eps_coef = (1. - alpha) / (1 - alpha_bar) ** 0.5\n",
    "    var = sigma2s.gather(-1, t)\n",
    "    mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n",
    "    return mean + (var ** 0.5) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_train(model, cond_embedder, device, loader, optimizer, loss, T, alpha_bar, epoch, reporter):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for (x0, c) in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x0 = x0.to(device)\n",
    "        # create xt\n",
    "        batch_size = x0.shape[0]\n",
    "        t = torch.randin(0, T, (batch_size,), device=device, dtype=torch.long)\n",
    "        noise = torch.rand_like(x0)\n",
    "        xt = q_sample(x0, t, alpha_bar, noise)\n",
    "        # create conditional embedding\n",
    "        c = c.to(device)\n",
    "        c = cond_embedder(c)\n",
    "        # compute backward noise prediction epsilon_theta\n",
    "        eps_theta = model(xt, t, c, device)\n",
    "        # compute loss between true noise and predicted noise\n",
    "        l = loss(noise, eps_theta)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += l.item()\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='train', loss=total_loss)\n",
    "    print(f\"Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_sample(model, embedder, device, shape, n_class, T, alpha_bars, alphas, sigma2s):\n",
    "    xt = torch.randn(shape).to(device)\n",
    "    for i in reversed(range(T)):\n",
    "        t = torch.tensor(i, device=device).repeat(shape[0])\n",
    "        c = torch.randin(0, n_class, shape[0]).to(device)\n",
    "        c = embedder(c, device)\n",
    "        eps_theta = model(xt, t, c)\n",
    "        xt = p_sample(xt, eps_theta, device, t, alpha_bars, alphas, sigma2s)\n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_train_sample(model, embedder, device, loader, optimizer, loss, T, shape, alpha_bars, alphas, sigma2s, total_epochs, reporter):\n",
    "    generated = []\n",
    "    for epoch in range(total_epochs):\n",
    "        sd_train(model, embedder, device, loader, optimizer, loss, T, alpha_bar, epoch, reporter)\n",
    "        x = sd_sample(model, embedder, device, shape, T, alpha_bars, alphas, sigma2s)\n",
    "        generated.append(x)\n",
    "    generated = torch.cat(generated, dim=0)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Stable Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
