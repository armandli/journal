{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import utils\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/armandli/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/Users/armandli/journal/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_built()\n",
    "if use_cuda:\n",
    "    device = torch.device('cuda')\n",
    "elif use_mps:\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_batch_size = 256\n",
    "loader_args = {'batch_size' : default_batch_size, 'shuffle' : True}\n",
    "score_args = {'batch_size' : default_batch_size, 'shuffle' : False}\n",
    "if use_cuda:\n",
    "    loader_args.update({'pin_memory' : True})\n",
    "    score_args.update({'pin_memory' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reporter(ABC):\n",
    "    @abstractmethod\n",
    "    def report(self, typ, **metric):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SReporter(Reporter):\n",
    "    def __init__(self):\n",
    "        self.log = []\n",
    "    def report(self, typ, **data):\n",
    "        self.log.append((typ, data))\n",
    "    def reset(self):\n",
    "        self.log.clear()\n",
    "    def loss(self, t):\n",
    "        losses = []\n",
    "        for (typ, data) in self.log:\n",
    "            if typ == t:\n",
    "                losses.append(data['loss'])\n",
    "        return losses\n",
    "    def loss(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data['loss']\n",
    "                    count -= 1\n",
    "        return float(\"inf\")\n",
    "    def eval_loss(self):\n",
    "        return self.loss('eval')\n",
    "    def train_loss(self):\n",
    "        return self.loss('train')\n",
    "    def eval_loss(self, idx):\n",
    "        return self.loss('eval', idx)\n",
    "    def train_loss(self, idx):\n",
    "        return self.loss('train', idx)\n",
    "    def get_record(self, t, idx):\n",
    "        if idx >= 0:\n",
    "            count = 0\n",
    "            for (typ, data) in self.log:\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count += 1\n",
    "        else:\n",
    "            count = -1\n",
    "            for (typ, data) in reversed(self.log):\n",
    "                if typ == t:\n",
    "                    if count == idx:\n",
    "                        return data\n",
    "                    count -= 1\n",
    "        return dict()\n",
    "    def eval_record(self, idx):\n",
    "        return self.get_record('eval', idx)\n",
    "    def train_record(self, idx):\n",
    "        return self.get_record('train', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset  = datasets.MNIST(root=data_dir, train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset = datasets.CIFAR10(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CelebA(root=data_dir, transform=transforms.ToTensor(), download=True)\n",
    "trainset, evalset = utils.data.random_split(dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR100(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)\n",
    "evalset = datasets.CIFAR100(root=data_dir, train=True, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(evalset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(dataset=trainset, **loader_args)\n",
    "eval_loader = utils.data.DataLoader(dataset=evalset, **score_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation():\n",
    "    return nn.ReLU(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling2DV2(in_c, out_c, stride, norm_layer):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 1, stride=stride),\n",
    "        norm_layer(out_c),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling2DV1(in_c, out_c, stride, norm_layer):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_c, out_c, 2, stride=stride),\n",
    "        norm_layer(out_c),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "    def forward(self, x):\n",
    "        a, b = torch.chunk(x, 2, dim=1)\n",
    "        return a * torch.sigmoid(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatELU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        #concat at channel dim\n",
    "        return F.elu(torch.cat([x, -x], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormLinear2d(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(WeightNormLinear2d, self).__init__()\n",
    "        self.layer = nn.utils.parametrizations.weight_norm(nn.Linear(d_in, d_out))\n",
    "        self.d_out = d_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        shape = [int(d) for d in x.shape]\n",
    "        x = self.layer(x.contiguous().view(shape[0]*shape[1]*shape[2], shape[3]))\n",
    "        shape[-1] = self.d_out\n",
    "        x = x.view(shape).permute(0, 3, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride=1, padding=0):\n",
    "        super(WeightNormConv2d, self).__init__()\n",
    "        self.layer = nn.utils.parametrizations.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormConvTransposed2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, output_padding=1):\n",
    "        super(WeightNormConvTransposed2d, self).__init__()\n",
    "        self.layer = nn.utils.parametrizations.weight_norm(nn.ConvTranspose2d(in_c, out_c, kernel_size, stride, output_padding=output_padding))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownShift(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DownShift, self).__init__()\n",
    "        #pad Left=0 Right=0 Up=1 Down=0\n",
    "        self.pad = nn.ZeroPad2d((0,0,1,0))\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = x[:, :, :shape[2]-1, :]\n",
    "        x = self.pad(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownShiftConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,3), stride=(1,1), shift_down=False):\n",
    "        super(DownShiftConv2d, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ZeroPad2d((int((kernel_size[1]-1)/2),int((kernel_size[1]-1)/2),kernel_size[0]-1,0)),\n",
    "            WeightNormConv2d(in_c, out_c, kernel_size, stride),\n",
    "        )\n",
    "        if shift_down:\n",
    "            self.shift_down = DownShift()\n",
    "        else:\n",
    "            self.shift_down = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.shift_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownShiftDeconv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,3), stride=(2,2)):\n",
    "        super(DownShiftDeconv2d, self).__init__()\n",
    "        self.ks = kernel_size\n",
    "        self.layer = WeightNormConvTransposed2d(in_c, out_c, kernel_size, stride, output_padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        s = x.shape\n",
    "        # correct the shape because TransposedConv2d would produce a few rows and columns bigger\n",
    "        x = x[:, :, :(s[2]-self.ks[0]+1), int((self.ks[1]-1)/2):(s[3]-int((self.ks[1]-1)/2))]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightShift(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RightShift, self).__init__()\n",
    "        #pad Left=1 Right=0 Up=0 Down=0\n",
    "        self.pad = nn.ZeroPad2d((1,0,0,0))\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = x[:, :, :, :shape[3]-1]\n",
    "        x = self.pad(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownRightShiftConv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,2), stride=(1,1), shift_right=False):\n",
    "        super(DownRightShiftConv2d, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ZeroPad2d((kernel_size[1]-1, 0, kernel_size[0]-1, 0)),\n",
    "            WeightNormConv2d(in_c, out_c, kernel_size, stride),\n",
    "        )\n",
    "        if shift_right:\n",
    "            self.shift_right = RightShift()\n",
    "        else:\n",
    "            self.shift_right = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.shift_right(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownRightShiftDeconv2d(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=(2,2), stride=(2,2)):\n",
    "        super(DownRightShiftDeconv2d, self).__init__()\n",
    "        self.ks = kernel_size\n",
    "        self.layer = WeightNormConvTransposed2d(in_c, out_c, kernel_size, stride, output_padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        s = x.shape\n",
    "        # correct the shape because TransposedConv2d produces a few rows and columns bigger\n",
    "        x = x[:, :, :(s[2]-self.ks[0]+1):, :(s[3]-self.ks[1]+1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualLayer(nn.Module):\n",
    "    def __init__(self, nc, conv, skip=0, p_dropout=0.5):\n",
    "        super(GatedResidualLayer, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            conv(2*nc, nc),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            nn.Dropout2d(p_dropout),\n",
    "            conv(2*nc, 2*nc),\n",
    "            Gate(),\n",
    "        )\n",
    "        if skip > 0:\n",
    "            self.skip = nn.Sequential(\n",
    "                ConcatELU(),\n",
    "                WeightNormLinear2d(2*skip*nc, nc),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, a=None):\n",
    "        s = x\n",
    "        x = self.layer1(x)\n",
    "        if a is not None:\n",
    "            x += self.skip(a)\n",
    "        x = self.layer2(x)\n",
    "        return s + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelUpSample(nn.Module):\n",
    "    def __init__(self, nlayers, nchannel):\n",
    "        super(PixelUpSample, self).__init__()\n",
    "        self.up_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownShiftConv2d, skip=0)\n",
    "            for _ in range(nlayers)\n",
    "        ])\n",
    "        self.upleft_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownRightShiftConv2d, skip=1)\n",
    "            for _ in range(nlayers)\n",
    "        ])\n",
    "        self.nlayers = nlayers\n",
    "    def forward(self, up, upleft):\n",
    "        ups, uplefts = [], []\n",
    "        for i in range(self.nlayers):\n",
    "            up = self.up_stream[i](up)\n",
    "            upleft = self.upleft_stream[i](upleft, a=up)\n",
    "            ups.append(up)\n",
    "            uplefts.append(upleft)\n",
    "        return ups, uplefts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDownSample(nn.Module):\n",
    "    def __init__(self, nlayer, nchannel):\n",
    "        super(PixelDownSample, self).__init__()\n",
    "        self.up_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownShiftConv2d, skip=1)\n",
    "            for _ in range(nlayer)\n",
    "        ])\n",
    "        self.upleft_stream = nn.ModuleList([\n",
    "            GatedResidualLayer(nchannel, DownRightShiftConv2d, skip=2)\n",
    "            for _ in range(nlayer)\n",
    "        ])\n",
    "        self.nlayer = nlayer\n",
    "    \n",
    "    def forward(self, up, upleft, ups, uplefts):\n",
    "        for i in range(self.nlayer):\n",
    "            up = self.up_stream[i](up, a=ups.pop())\n",
    "            upleft = self.upleft_stream[i](upleft, a=torch.cat((up, uplefts.pop()), 1))\n",
    "        return up, upleft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer2DV4(nn.Module):\n",
    "    def __init__(self, in_c, out_c, ksz, act_layer, norm_layer, stride=1):\n",
    "        super(ResidualLayer2DV4, self).__init__()\n",
    "        if in_c <= out_c:\n",
    "            self.c1 = nn.Conv2d(in_c, out_c, ksz, stride=stride, padding=int((ksz-1)/2))\n",
    "            self.c2 = nn.Conv2d(out_c, out_c, ksz, stride=1, padding=int((ksz-1)/2))\n",
    "        else:\n",
    "            self.c1 = nn.ConvTranspose2d(in_c, out_c, ksz+1, stride=stride, padding=int((ksz-1)/2))\n",
    "            self.c2 = nn.ConvTranspose2d(out_c, out_c, ksz, stride=1, padding=int((ksz-1)/2))\n",
    "        self.a1 = act_layer()\n",
    "        self.a2 = act_layer()\n",
    "        self.b1 = norm_layer(in_c)\n",
    "        self.b2 = norm_layer(out_c)\n",
    "        \n",
    "        if in_c < out_c:\n",
    "            self.residual = downsampling2DV2(in_c, out_c, stride, norm_layer)\n",
    "        elif in_c > out_c:\n",
    "            self.residual = upsampling2DV1(in_c, out_c, stride, norm_layer)\n",
    "        elif stride > 1:\n",
    "            self.residual = downsampling2DV2(in_c, out_c, stride, norm_layer)\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x\n",
    "        x = self.b1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.c1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.c2(x)\n",
    "        s = self.residual(s)\n",
    "        x = x + s\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVariationalEncoderV2(nn.Module):\n",
    "    def __init__(self, ic, chmuls, hmul):\n",
    "        super(ConvVariationalEncoderV2, self).__init__()\n",
    "        layer1 = []\n",
    "        outmul = 1\n",
    "        for mul in chmuls:\n",
    "            layer1.append(ResidualLayer2DV4(ic*outmul, ic*mul, 3, relu_activation, nn.BatchNorm2d, stride=2))\n",
    "            outmul = mul\n",
    "        self.layer1 = nn.ModuleList(layer1)\n",
    "        self.mu_layer = nn.Conv2d(ic*outmul, ic*hmul, (3,3), (2,2), (1,1))\n",
    "        self.sig_layer = nn.Sequential(\n",
    "            nn.Conv2d(ic*outmul, ic*hmul, (3,3), (2,2), (1,1)),\n",
    "            nn.Softplus(threshold=6),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layer1:\n",
    "            x = layer(x)\n",
    "        mu = self.mu_layer(x)\n",
    "        sig = self.sig_layer(x)\n",
    "        return (mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, in_c, nresnet, nlayer, nchannel=80, nlogmix=10):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        down_nlayer = [nresnet] + [nresnet+1 for _ in range(1, nlayer)]\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            PixelDownSample(down_nlayer[i], nchannel) for i in range(nlayer)\n",
    "        ])\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            PixelUpSample(nresnet, nchannel) for _ in range(nlayer)\n",
    "        ])\n",
    "        self.downsize_up_stream = nn.ModuleList([\n",
    "            DownShiftConv2d(nchannel, nchannel, stride=(2,2)) for _ in range((nlayer-1))\n",
    "        ])\n",
    "        self.downsize_upleft_stream = nn.ModuleList([\n",
    "            DownRightShiftConv2d(nchannel, nchannel, stride=(2,2)) for _ in range(nlayer-1)\n",
    "        ])\n",
    "        self.upsize_up_stream = nn.ModuleList([\n",
    "            DownShiftDeconv2d(nchannel, nchannel, stride=(2,2)) for _ in range(nlayer-1)\n",
    "        ])\n",
    "        self.upsize_upleft_stream = nn.ModuleList([\n",
    "            DownRightShiftDeconv2d(nchannel, nchannel, stride=(2,2)) for _ in range(nlayer-1)\n",
    "        ])\n",
    "        self.up_init = DownShiftConv2d(in_c+1, nchannel, kernel_size=(2,3), shift_down=True)\n",
    "        self.upleft_init = nn.ModuleList([\n",
    "            DownShiftConv2d(in_c+1, nchannel, kernel_size=(1,3), shift_down=True),\n",
    "            DownRightShiftConv2d(in_c+1, nchannel, kernel_size=(2,1), shift_right=True),\n",
    "        ])\n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            WeightNormLinear2d(nchannel, nlogmix*(in_c*3+1)),\n",
    "        )\n",
    "        self.nlayer = nlayer\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        shape = x.shape\n",
    "        padding = torch.ones(shape[0], 1, shape[2], shape[3], device=device, requires_grad=False)\n",
    "        x = torch.cat((x, padding), 1)\n",
    "        \n",
    "        # UP PASS\n",
    "        ups = [self.up_init(x)]\n",
    "        uplefts = [self.upleft_init[0](x) + self.upleft_init[1](x)]\n",
    "        for i in range(self.nlayer):\n",
    "            up_out, upleft_out = self.up_layers[i](ups[-1], uplefts[-1])\n",
    "            ups.extend(up_out)\n",
    "            uplefts.extend(upleft_out)\n",
    "            if i < self.nlayer-1:\n",
    "                ups.append(self.downsize_up_stream[i](ups[-1]))\n",
    "                uplefts.append(self.downsize_upleft_stream[i](uplefts[-1]))\n",
    "\n",
    "        # DOWN PASS\n",
    "        up = ups.pop()\n",
    "        upleft = uplefts.pop()\n",
    "        for i in range(self.nlayer):\n",
    "            up, upleft = self.down_layers[i](up, upleft, ups, uplefts)\n",
    "            if i < self.nlayer-1:\n",
    "                up = self.upsize_up_stream[i](up)\n",
    "                upleft = self.upsize_upleft_stream[i](upleft)\n",
    "        \n",
    "        x = self.out_layer(upleft)\n",
    "        return x\n",
    "\n",
    "    def sample(self, batch_sz, img_shape, device):\n",
    "        x = torch.zeros(batch_sz, img_shape[0], img_shape[1], img_shape[2]).to(device)\n",
    "        shape = x.shape\n",
    "        padding = torch.ones(shape[0], 1, shape[2], shape[3], device=device, requires_grad=False)\n",
    "        x = torch.cat((x, padding), 1)\n",
    "        \n",
    "        # UP PASS\n",
    "        ups = [self.up_init(x)]\n",
    "        uplefts = [self.upleft_init[0](x) + self.upleft_init[1](x)]\n",
    "        for i in range(self.nlayer):\n",
    "            up_out, upleft_out = self.up_layers[i](ups[-1], uplefts[-1])\n",
    "            ups.extend(up_out)\n",
    "            uplefts.extend(upleft_out)\n",
    "            if i < self.nlayer-1:\n",
    "                ups.append(self.downsize_up_stream[i](ups[-1]))\n",
    "                uplefts.append(self.downsize_upleft_stream[i](uplefts[-1]))\n",
    "\n",
    "        # DOWN PASS\n",
    "        up = ups.pop()\n",
    "        upleft = uplefts.pop()\n",
    "        for i in range(self.nlayer):\n",
    "            up, upleft = self.down_layers[i](up, upleft, ups, uplefts)\n",
    "            if i < self.nlayer-1:\n",
    "                up = self.upsize_up_stream[i](up)\n",
    "                upleft = self.upsize_upleft_stream[i](upleft)\n",
    "        \n",
    "        x = self.out_layer(upleft)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelDecoderV1(nn.Module):\n",
    "    def __init__(self, ic, chmuls, hmul, n_res, n_layer, nmix=10):\n",
    "        super(PixelDecoderV1, self).__init__()\n",
    "        layers = []\n",
    "        outmul = hmul\n",
    "        for mul in reversed(chmuls):\n",
    "            layers.append(ResidualLayer2DV4(ic*outmul, ic*mul, 3, relu_activation, nn.BatchNorm2d, stride=2))\n",
    "            outmul = mul\n",
    "        layers.append(ResidualLayer2DV4(ic*outmul, ic, 3, relu_activation, nn.BatchNorm2d, stride=2))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.pixel = PixelCNN(in_c=ic, nresnet=n_res, nlayer=n_layer, nlogmix=nmix)\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.pixel(x, device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelVariationalAutoEncoderV1(nn.Module):\n",
    "    def __init__(self, ic, chmuls, hmul, dist, n_res=1, n_layer=1, nmix=10):\n",
    "        super(PixelVariationalAutoEncoderV1, self).__init__()\n",
    "        self.encoder = ConvVariationalEncoderV2(ic, chmuls, hmul)\n",
    "        self.decoder = PixelDecoderV1(ic, chmuls, hmul, n_res, n_layer, nmix)\n",
    "        self.dist = dist\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        mu, sig = self.encode(x)\n",
    "        x_h = self.decode(mu, sig, device)\n",
    "        return (x_h, mu, sig)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, sig = self.encoder(x)\n",
    "        return (mu, sig)\n",
    "    \n",
    "    def decode(self, mu, sig, device):\n",
    "        s = self.dist.sample(mu.shape).to(device)\n",
    "        z = mu + sig * s\n",
    "        x_h = self.decoder(z, device)\n",
    "        return x_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedMixLogisticLoss(nn.Module):\n",
    "    def __init__(self, n_channel, nmix):\n",
    "        super(DiscretizedMixLogisticLoss, self).__init__()\n",
    "        self.nc = n_channel\n",
    "        self.nmix = nmix\n",
    "    \n",
    "    def log_sum_exp(self, x):\n",
    "        axis = len(x.shape)-1\n",
    "        m, _ = torch.max(x, dim=axis)\n",
    "        n, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "        return m + torch.log(torch.sum(torch.exp(x-n), dim=axis))\n",
    "\n",
    "    def log_prob_from_logits(self, x):\n",
    "        axis = len(x.shape)-1\n",
    "        m, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "        return x - m - torch.log(torch.sum(torch.exp(x-m), dim=axis, keepdim=True))\n",
    "    \n",
    "    def forward(self, target, prediction, device):\n",
    "        nmix = self.nmix\n",
    "        target = target.permute(0, 2, 3, 1)\n",
    "        prediction = prediction.permute(0, 2, 3, 1)\n",
    "        ts = list(target.shape)\n",
    "\n",
    "        # unpack prediction parameters\n",
    "        lp = prediction[:,:,:,:nmix]\n",
    "        prediction = prediction[:,:,:,nmix:].view(ts+[nmix*3]) # 3 for mean, scale, coeff\n",
    "        means = prediction[:,:,:,:,:nmix]\n",
    "        log_scales = torch.clamp(prediction[:,:,:,:,nmix:nmix*2], min=-7.)\n",
    "        coeffs = torch.tanh(prediction[:,:,:,:,nmix*2:nmix*3])\n",
    "        target = target.unsqueeze(-1) + torch.zeros(ts+[nmix], requires_grad=False).to(device)\n",
    "        \n",
    "        ms = []\n",
    "        for i in range(self.nc):\n",
    "            if i == 0:\n",
    "                ms.append(means[:,:,:,0,:].unsqueeze(3))\n",
    "            elif i == 1:\n",
    "                ms.append((means[:,:,:,1,:]+coeffs[:,:,:,0,:]*target[:,:,:,0,:]).view(ts[0],ts[1],ts[2],1,nmix))\n",
    "            elif i == 2:\n",
    "                ms.append((means[:,:,:,2,:]+coeffs[:,:,:,1,:]*target[:,:,:,0,:]+coeffs[:,:,:,2,:]*target[:,:,:,1,:]).view(ts[0],ts[1],ts[2],1,nmix))\n",
    "            else:\n",
    "                assert False\n",
    "        \n",
    "        centered_target = target - torch.cat(ms, dim=3)\n",
    "        inv_stdv  = torch.exp(-log_scales)\n",
    "        plus_in   = inv_stdv * (centered_target + 1./255.)\n",
    "        cdf_plus  = torch.sigmoid(plus_in)\n",
    "        min_in    = inv_stdv * (centered_target - 1./255.)\n",
    "        cdf_minus = torch.sigmoid(min_in)\n",
    "\n",
    "        log_cdf_plus          = plus_in - F.softplus(plus_in)\n",
    "        log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "        cdf_delta             = cdf_plus - cdf_minus\n",
    "        log_pdf_mid = (inv_stdv*centered_target) - log_scales - 2.*F.softplus(inv_stdv*centered_target)\n",
    "\n",
    "        inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "        inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1-inner_inner_cond)*(log_pdf_mid-math.log(127.5))\n",
    "        inner_cond       = (target > 0.999).float()\n",
    "        inner_out        = inner_cond*log_one_minus_cdf_min + (1.-inner_cond)*inner_inner_out\n",
    "        cond             = (target < -0.999).float()\n",
    "        log_probs        = cond*log_cdf_plus + (1.-cond)*inner_out\n",
    "        log_probs        = torch.sum(log_probs, dim=3) + self.log_prob_from_logits(lp)\n",
    "        return -torch.sum(self.log_sum_exp(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelVAELoss(nn.Module):\n",
    "    def __init__(self, n_channel, nmix):\n",
    "        super(PixelVAELoss, self).__init__()\n",
    "        self.mixlogloss = DiscretizedMixLogisticLoss(n_channel, nmix)\n",
    "    \n",
    "    def forward(self, target, prediction, mu, sig, device):\n",
    "        recon_loss = self.mixlogloss(target, prediction, device)\n",
    "        dkl_loss = (sig**2. + mu**2. - torch.log(sig) - 0.5).sum()\n",
    "        return recon_loss + dkl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_train(model, device, loader, optimizer, loss, epoch, reporter):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for x, _ in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        x_h, mu, sig = model(x, device)\n",
    "        l = loss(x, x_h, mu, sig, device)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += l.item()\n",
    "        #print(f\"Epoch {epoch}: {l.item()}\")\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='train', loss=total_loss)\n",
    "    print(f\"Train Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_validate(model, device, loader, loss, train_epoch, reporter):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            x_h, mu, sig = model(x, device)\n",
    "            total_loss += loss(x, x_h, mu, sig, device)\n",
    "    total_loss /= float(len(loader.dataset))\n",
    "    reporter.report(typ='eval', loss=total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_image_train_validate(\n",
    "        model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        eval_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loss,\n",
    "        total_epoch,\n",
    "        patience,\n",
    "        patience_decay,\n",
    "        reporter,\n",
    "):\n",
    "    validation_loss = float(\"inf\")\n",
    "    patience_count = patience\n",
    "    patience = int(patience * patience_decay)\n",
    "    reset_patience = False\n",
    "    for epoch in range(total_epoch):\n",
    "        vae_image_train(model, device, train_loader, optimizer, loss, epoch, reporter)\n",
    "        vae_image_validate(model, device, eval_loader, loss, epoch, reporter)\n",
    "        new_validation_loss = reporter.eval_loss(-1)\n",
    "        print(f\"Epoch {epoch} VLoss: {new_validation_loss}\")\n",
    "        scheduler.step(new_validation_loss)\n",
    "        if new_validation_loss < validation_loss:\n",
    "            validation_loss = new_validation_loss\n",
    "            patience_count = patience\n",
    "            if reset_patience:\n",
    "                patience = int(patience * patience_decay)\n",
    "                reset_patience = False\n",
    "        else:\n",
    "            validation_loss = new_validation_loss\n",
    "            patience_count -= 1\n",
    "            reset_patience = True\n",
    "            if patience_count <= 0:\n",
    "                print(f\"Improvement stopped. VLoss: {validation_loss}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dist = distributions.Normal(0., 1.)\n",
    "inc = trainset[0][0].shape[0]\n",
    "nmix=10\n",
    "model = PixelVariationalAutoEncoderV1(inc, [inc*2], inc*4, norm_dist, n_res=1, n_layer=1, nmix=nmix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "total_epochs = 60\n",
    "patience = 16\n",
    "patience_decay = 0.9\n",
    "optimizer = optim.Adam(model.parameters(recurse=True), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, threshold=0.000001)\n",
    "loss = PixelVAELoss(inc, nmix)\n",
    "reporter = SReporter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1323230.25\n",
      "Epoch 0: 1320879.25\n",
      "Epoch 0: 1319263.625\n",
      "Epoch 0: 1320859.625\n",
      "Epoch 0: 1320278.75\n",
      "Epoch 0: 1318593.5\n",
      "Epoch 0: 1315683.5\n",
      "Epoch 0: 1317102.25\n",
      "Epoch 0: 1314431.375\n",
      "Epoch 0: 1316079.0\n",
      "Epoch 0: 1314233.25\n",
      "Epoch 0: 1311130.875\n",
      "Epoch 0: 1311594.625\n",
      "Epoch 0: 1309382.0\n",
      "Epoch 0: 1308084.5\n",
      "Epoch 0: 1307314.75\n",
      "Epoch 0: 1307985.375\n",
      "Epoch 0: 1305899.0\n",
      "Epoch 0: 1307637.125\n",
      "Epoch 0: 1305111.0\n",
      "Epoch 0: 1296954.375\n",
      "Epoch 0: 1303247.625\n",
      "Epoch 0: 1302008.0\n",
      "Epoch 0: 1301783.375\n",
      "Epoch 0: 1300403.0\n",
      "Epoch 0: 1297604.0\n",
      "Epoch 0: 1298394.5\n",
      "Epoch 0: 1297175.875\n",
      "Epoch 0: 1296430.375\n",
      "Epoch 0: 1297118.625\n",
      "Epoch 0: 1294054.75\n",
      "Epoch 0: 1290349.25\n",
      "Epoch 0: 1291157.0\n",
      "Epoch 0: 1287990.0\n",
      "Epoch 0: 1287996.375\n",
      "Epoch 0: 1290241.125\n",
      "Epoch 0: 1288105.5\n",
      "Epoch 0: 1288439.375\n",
      "Epoch 0: 1282692.0\n",
      "Epoch 0: 1285247.125\n",
      "Epoch 0: 1285459.375\n",
      "Epoch 0: 1281608.0\n",
      "Epoch 0: 1279849.375\n",
      "Epoch 0: 1277895.125\n",
      "Epoch 0: 1277721.75\n",
      "Epoch 0: 1276274.375\n",
      "Epoch 0: 1273270.875\n",
      "Epoch 0: 1274392.25\n",
      "Epoch 0: 1272919.125\n",
      "Epoch 0: 1271459.0\n",
      "Epoch 0: 1271179.75\n",
      "Epoch 0: 1269040.0\n",
      "Epoch 0: 1267655.75\n",
      "Epoch 0: 1264608.625\n",
      "Epoch 0: 1261856.25\n",
      "Epoch 0: 1266643.75\n",
      "Epoch 0: 1262587.875\n",
      "Epoch 0: 1261477.25\n",
      "Epoch 0: 1256908.375\n",
      "Epoch 0: 1258701.5\n",
      "Epoch 0: 1257429.75\n",
      "Epoch 0: 1252856.75\n",
      "Epoch 0: 1250069.25\n",
      "Epoch 0: 1251958.375\n",
      "Epoch 0: 1249292.5\n",
      "Epoch 0: 1245077.375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/armandli/journal/diffusion/vae3.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vae_image_train_validate(model, device, train_loader, eval_loader, optimizer, scheduler, loss, total_epochs, patience, patience_decay, reporter)\n",
      "\u001b[1;32m/Users/armandli/journal/diffusion/vae3.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m reset_patience \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     vae_image_train(model, device, train_loader, optimizer, loss, epoch, reporter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     vae_image_validate(model, device, eval_loader, loss, epoch, reporter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     new_validation_loss \u001b[39m=\u001b[39m reporter\u001b[39m.\u001b[39meval_loss(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/armandli/journal/diffusion/vae3.ipynb Cell 47\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x_h, mu, sig \u001b[39m=\u001b[39m model(x, device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m l \u001b[39m=\u001b[39m loss(x, x_h, mu, sig, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m l\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armandli/journal/diffusion/vae3.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/journal/env/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/journal/env/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae_image_train_validate(model, device, train_loader, eval_loader, optimizer, scheduler, loss, total_epochs, patience, patience_decay, reporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = model_dir + 'vaev3_mnist_v1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = model_dir + 'vaev3_cifar10_v1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = distributions.Normal(0., 1.)\n",
    "inc = trainset[0][0].shape[0]\n",
    "model = PixelVariationalAutoEncoderV1(ic=inc, chmuls=[inc*2, inc*4], hmul=inc*8, dist=dist, n_res=1, n_layer=3, nmix=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(filename, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixLogisticToColorChannel(nn.Module):\n",
    "    def __init__(self, n_channel, nmix):\n",
    "        super(MixLogisticToColorChannel, self).__init__()\n",
    "        self.nc = n_channel\n",
    "        self.nmix = nmix\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        nmix = self.nmix\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        xs = list(x.shape[:-1]) + [self.nc]\n",
    "        # unpack parameters\n",
    "        p = x[:,:,:,:nmix]\n",
    "        x = x[:,:,:,nmix:].view(xs + [nmix*3])\n",
    "        # sample mixture indicator from softmax\n",
    "        temp = torch.FloatTensor(p.shape).to(device)\n",
    "        temp.uniform_(1e-5, 1.-1e-5)\n",
    "        temp = p - torch.log(-torch.log(temp))\n",
    "        _, argmax = temp.max(dim=3)\n",
    "        \n",
    "        one_hot = torch.FloatTensor(argmax.size()+(nmix,)).zero_().to(device)\n",
    "        one_hot.scatter_(len(argmax.size()), argmax.unsqueeze(-1), 1.)\n",
    "        sel = one_hot.view(xs[:-1]+[1, nmix])\n",
    "        #select logistic parameter\n",
    "        means = torch.sum(x[:,:,:,:,:nmix] * sel, dim=4)\n",
    "        log_scales = torch.clamp(torch.sum(x[:,:,:,:,nmix:2*nmix]*sel, dim=4), min=-7.)\n",
    "        coeffs = torch.sum(torch.tanh(x[:,:,:,:,2*nmix:3*nmix]*sel), dim=4)\n",
    "        # sample from logistic & clip to interval\n",
    "        # don't actually round to nearest 8 bit value when sampling\n",
    "        u = torch.FloatTensor(means.shape).to(device)\n",
    "        u.uniform_(1e-5, 1.-1e-5)\n",
    "        x = means + torch.exp(log_scales)*(torch.log(u)-torch.log(1.-u))\n",
    "\n",
    "        x_lst = []\n",
    "        for i in range(self.nc):\n",
    "            if i == 0:\n",
    "                x0 = torch.clamp(x[:,:,:,0], min=-1., max=1.)\n",
    "                x_lst.append(x0.view(xs[:-1]+[1]))\n",
    "            elif i == 1:\n",
    "                x1 = torch.clamp(x[:,:,:,1]+coeffs[:,:,:,0]*x0, min=-1., max=1.)\n",
    "                x_lst.append(x1.view(xs[:-1]+[1]))\n",
    "            elif i == 2:\n",
    "                x2 = torch.clamp(x[:,:,:,2]+coeffs[:,:,:,1]*x0+coeffs[:,:,:,2]*x1, min=-1., max=1.)\n",
    "                x_lst.append(x2.view(xs[:-1]+[1]))\n",
    "            else:\n",
    "                assert False\n",
    "\n",
    "        x = torch.cat(x_lst, dim=3)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_img = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(dataset):\n",
    "    i = random.randint(0, len(dataset))\n",
    "    return dataset[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample_image(evalset)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_h, _, _ = model(torch.unsqueeze(x, dim=0).to(device), device)\n",
    "m = MixLogisticToColorChannel(n_channel=x.shape[0], nmix=10)\n",
    "x_h = m(x_h, device)\n",
    "x_h = x_h.reshape(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACiklEQVR4nAXBAWgUZACA0f9bQ9pmmm6Cijq7GDokzWMFbdJMUOrYyPAYw7mWa7WQTcG5dGmbkUipoGY6WzFit8gw0WkzlqGQGdzM2ZwzBGkthmlMM+dQsPb1Xqjhr2gO3K//kNftyI6rT0/UCmVveCs6+AS2TnPBjDxERz3gIXg1p25VoNXIvKJqXrbsY05Znb6ARltEywkwANo5yPVNsgd0dA4NGSLB3GladCtpW4bslff0gWJ3VUFF2NCkA0OUyiUD/KgdIL/uS2DYw7GLoxvX7lZKLMtYX6TbuNJTUbwuNpL6VHglpN9LSb89PJby/d9h+PHW6NLKlLHff3iteEKY6Pi+aL7OKLCB425JpC4vR6yFZcFBvlhoxFwmLB5XaJDZL66EhfUDxAIvsf2Dxk+yH8161mcwWfTPzGXtTnqX5ocG115YfNfSYS7HBWGJahMtkyV4c4OixHjBn3aowHfoZxBk/G9nE81YE7Jc90vDavF5VpmifwSZBeV5kzi8ElVKEb5FDraFRotN3HlTvOhZINO+bTDv9OX4lLfDaaxaoj53AVHO7Xpnju7/vG7qekLX0p3wH+XXaq862jn/Z1206QScp91IOMOXX1OwM69G4frsNJRvOhrU7fmBk2/syzbtTH1Sm30kdoshxDX8u8tg/IFw7r70Jjbu7xFcJFm0fcTh0IV2O1LDAfIV0KrHVnR+xdYjhOP92vT+n9hfYv+N2jJJ89Dc/idRgzdmdjt1eWYbyWvq5pZLW3Z/qqtX2HMkWMcIAh7rPVnxcApA1OnGmB4JehPG0PpIeweyWWZxB+4dpStcJSfpbRsracm0LxW29lpZTYlxCgPCKc0Vdb4HuTtUch6kEP8HYSqM9SOsmIcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_img(x_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3ElEQVR4nMWQMUsDQRSEJ4uFhT8gXewsFWwEi+QXBCstL3/AFCmi1pf+yhSBg6S3sBcR/AWBEAhaiU0KwfJgmDuLM3tc7p1lMs2+3W/evOUB+1emx6MadBiKbNXAiCL7JjqLEop8NuELSZFflwbrSdKbJBlwSXLUvCZ5X2FXIt9PcUwy/ntyHnYB3MxLfle6fZfDDjZFJ3De3tiGSJFO18BdCmRW7GcCnAB4MmfmGv4H/beKmc4BwKSN11nFfvEjfpyHNYuPxXzxt0WYr1b5kQzGRieCBUk9WGiH+gUF0lxk324NAQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_img(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
